00:00 Hello, I'm Luke and welcome to my 
podcast for learners of English   all around the world. This episode is 
called Technology in Everyday Life,  
00:09 the choices we make, topic, discussion and 
vocabulary. And this episode is all about   the choices we have to make relating 
to technology in our everyday lives.
00:19 It will include loads of vocabulary and 
will give you the chance not only to   listen to me talking about this, but also 
to practise your speaking on this subject  
00:29 too. Technology is a hugely dominant 
force in our lives today. More than  
00:34 at any other point in human history, 
we rely on it for almost everything. Just think about how often you 
reach for your phone every day.  
00:43 Consider how many of the things we 
do depend on an internet connection   or even just electricity. Our phones 
have become our constant companions.
00:52 We live in a world where everything is connected 
and everything is at our fingertips. Meanwhile,  
00:60 AI is rapidly emerging as the most powerful 
technology since the invention of the combustion  
01:05 engine and is set to bring profound changes. 
All of this has brought us incredible benefits,  
01:12 but at what cost? Every interaction we have 
with technology involves a series of choices.
01:20 We constantly have to weigh up the advantages 
against the personal, social and environmental  
01:26 costs of using all the various forms of tech 
which have found their way into our everyday  
01:32 lives. This is a crucial topic and one which comes 
into our conversations all the time these days.  
01:38 So with this episode, I want to give you the 
tools to have these conversations in English.
01:45 So coming up in this episode, I have a list 
of discussion questions relating to different   aspects of technology in our everyday 
lives. These are questions I've used in  
01:55 class with my students for discussion 
practise and also for vocabulary work  
01:60 and now I'm converting that into a podcast 
episode. You can use these questions too.
02:06 You can just say your answers out loud 
either with a speaking partner or on   your own. They are talking points which 
you can use to practise your speaking on  
02:16 this subject. You can find them all 
listed on the PDF for this episode. There's a link in the description for the 
PDF and yes, that PDF has all the notes from  
02:29 this episode including discussion questions, 
vocabulary lists and also a full transcript.  
02:37 This episode also has full English subtitles 
on YouTube so you can switch them on if you  
02:43 want to read at the same time as watching this. 
If you're listening to this in a podcast app,   as I said, you'll find the full 
transcript link in the description.
02:52 So I'll demonstrate how you can use 
these questions for speaking practise   on this topic and after listening to 
this, use the questions to do the same  
03:02 thing yourself. Notice the vocabulary 
that I'm using and use it yourself while  
03:07 you practise your speaking skills. This 
episode is listening practise but also  
03:12 a chance to spot different bits of 
vocabulary which you can use later. There'll be words and expressions on all the main 
topics relating to how we use technology in our  
03:23 daily lives and I'm talking about issues such as 
these privacy versus convenience, data sharing,  
03:30 digital detox, online behaviour, digital 
legacy, tech addiction, tech for children,  
03:37 information quality, digital sustainability, AI 
and automation, security practises, surveillance  
03:45 and privacy, tech company ethics and tech and 
wellbeing. So what do you think? Would you be able  
03:52 to discuss these things fully in English? Do you 
know about the various issues connected to these  
03:58 things and do you know all the relevant words that 
you would need to use when having a conversation  
04:04 on any of those subjects? Well that's what 
this episode will help you with. Now I'm going  
04:09 to answer these questions as a lay person and I 
should probably explain what a lay person means.
04:16 A lay person not as an expert. So 
we use the word lay person when  
04:22 we're talking about science or maybe 
other areas of expertise like the law,  
04:28 technology in this case. A lay person is 
just a normal person not an expert, okay.
04:33 So sometimes we use the word layman in layman's 
terms but also we can say lay person and it just  
04:41 means someone who doesn't have expert knowledge in 
the field. So for example if you're talking about   the law, if you're talking about legal issues, 
you could have legal specialists, lawyers who  
04:51 have special knowledge about a certain subject and 
then you've got the lay person or the layman and  
04:57 that would be just someone who doesn't really 
have special inside knowledge on the subject.   And so it's just kind of like talking in general 
terms, it's the same thing applies to science.
05:06 You can talk about scientific subjects in 
that kind of specialist scientific way or  
05:11 you can talk about things in a more general 
way that everyone could understand even if  
05:17 you don't have specialist knowledge. 
So that would be in layman's terms   or talking as a lay person. So that's what 
I'm going to be doing during this episode.
05:26 I'll be answering those questions, talking 
about them as a lay person, not someone with  
05:31 expert knowledge and I'm assuming that you would 
be discussing these issues from that same position  
05:38 too, unless of course you are an expert. But 
I'm assuming that most of you listening to   this are not sort of a tech expert, but you are 
someone who obviously lives with technology and  
05:49 lives with all of the effects of it and all of the 
complex choices that relate to the way we use it,  
05:54 right? But anyway, please don't expect to get the 
expert answers to these questions in many cases,  
06:02 there is no perfect answer in fact, but really 
this is all more a question of how we describe and  
06:08 discuss technology today and how it influences our 
lives as ordinary people. Just before we begin,  
06:15 I need to remind myself to set a 30 minute 
timer to make sure I stop for some water.
06:21 So let me do that right now. That's 
that timer has started. Also,   I want to remember to make notes of vocabulary 
on a separate document during the episode,  
06:32 because no doubt, bits of vocab 
will come up during this episode. And I would like to remember to make a note 
of that vocabulary on a different document.  
06:42 I've already started in fact, and here 
in my notes, I've got some bits of vocab,  
06:48 which I've already used in fact. So we have 
vocab like technology is a hugely dominant force.
06:56 It's dominant. It dominates our lives. We rely 
on it for almost everything, meaning we need it.
07:02 We are reliant on it. We rely on it means, 
you know, we need it for various different  
07:08 things. Consider how many of the things 
we do depend on an internet connection.
07:15 So we depend on it. We rely on it. We need it. These are all synonyms. Everything is at our 
fingertips. If something is at your fingertips,  
07:24 it's it just means that it's it's right there   available to you easily and immediately in 
this, you know, with our phones, of course,  
07:35 literally things are at our fingertips 
because all you need to do is go on your   phone, open up your internet browser and 
literally things are at your fingertips.
07:45 So that's just an expression that means things 
are close and near to you and available to you  
07:50 instantly. AI is rapidly emerging 
as the most powerful technology,  
07:55 blah, blah, blah, rapidly emerging. If 
something emerges, it means it arrives. It becomes real, becomes visible. OK, 
it becomes obvious and AI is emerging,  
08:09 meaning it's coming out as or revealing itself 
to be very powerful. It's rapidly emerging.
08:16 Just means it's it's arriving very 
quickly. We constantly have to weigh   up the advantages against the costs to 
weigh something up. You can imagine one of  
08:27 those old fashioned systems of measuring, 
let's say, ingredients for making food.
08:32 You would use scales to weigh something to 
measure the weight of it. And we weigh things up,  
08:40 just means that we kind of measure the weight 
in comparison to each other. So if you put the  
08:46 costs on one side of the scale and the 
benefits on the other side of the scale,  
08:51 we then weigh up those two things and see 
which one is greater than the other one, right?  
08:57 Measure their weight in comparison 
to each other or basically see   the importance, the relative 
importance of those things.
09:04 We have to weigh up the advantages against the 
costs. This is a crucial topic, crucial, meaning  
09:11 very important. And yes, I'm going to answer 
these questions as a layperson, not as an expert.
09:18 And I've already explained that. 
So I'm going to try and keep this   list of vocab going. There will be 
other bits of vocab in the document.
09:25 But anyway, let's continue. 
I'll be putting a big list,   a big vocab list at the end 
of the PDF for this. Right.
09:31 So let's start with the first topic here, which is 
privacy versus convenience. And I should probably  
09:38 highlight the pronunciation of the word privacy. 
So in British English, and I'm from the UK,  
09:45 so I speak British English, we pronounce it 
privacy, whereas in American English, it would  
09:50 be pronounced privacy, which I kind of understand 
because the other word, the adjective is private,  
09:57 right? Something is private or public, right? 
Private is how the adjective is pronounced.
10:04 And so it makes a bit more sense to pronounce 
the noun as privacy. But nevertheless,  
10:11 in British English, we pronounce the noun as   privacy and the adjective privacy. So 
anyway, privacy versus convenience.
10:19 So again, weighing up these things in today's 
hyper connected world, we constantly weigh the  
10:27 benefits of convenience against the risks to 
our privacy. So weighing up the benefits of  
10:35 the convenience of being able to use, for example, 
so many different apps on our phones against the  
10:41 risks of that to our privacy, right? So we're 
talking about data really, signing up for apps,  
10:49 smart home devices like your Amazon Alexa, your 
smart speaker, signing up for these things or  
10:56 signing up for loyalty programmes online often 
means handing over personal data. This data  
11:04 might improve our experience, making services 
faster, more personalised or even cheaper,  
11:10 but it also exposes us to risks like data misuse, 
targeted advertising and even identity theft.
11:18 Targeted advertising is probably the least 
serious of all these things. It's not that bad,  
11:25 but nevertheless, whenever we sign up for some 
thing, you know, whether it's an app or some  
11:32 service, often the trade that we make is that 
we allow that service to access so much of our  
11:39 data. And this includes, well, many things like, 
for example, if you are, if you're signing up to  
11:46 a smart speaker, now obviously you can talk 
to that smart speaker and it will help you  
11:52 do things like browse the internet or play music 
or listen to the radio or podcasts or something.
11:60 But often there will be small print, terms 
and conditions, which when you read them,  
12:07 make you realise that actually, okay, so you're 
also allowing that smart speaker to collect a  
12:13 lot of information on you, maybe even listening 
in on your conversations and collect that data,  
12:20 which it's, suppose it's they say it's being used 
to provide a service to you, but let's be honest,  
12:28 a lot of the time, what these companies are doing 
is they're taking as much information about you as  
12:33 they possibly can, not just to provide you with a 
good service, but also so they could package that  
12:40 information and sell it off to third parties, 
perhaps so that they can target advertising at  
12:47 you more effectively, but maybe for all sorts 
of other things that we're not completely aware  
12:53 of. So the question is, how much privacy are 
we willing to sacrifice for convenience? And  
12:59 the questions then to discuss, do you always 
accept cookies when browsing websites? Do you  
13:05 always read the terms and conditions before 
clicking I accept? And are there any apps or  
13:11 services which you don't use or don't like, 
because you're concerned about how they use  
13:16 your data? All right, pretty big questions. 
So if I just answer these questions myself,  
13:24 as a quote unquote normal person, this 
is the sort of thing that you can do too.
13:30 So I would like you to try and answer 
those questions yourselves. But anyway,   this is how I would answer 
those things. So do I always  
13:38 accept cookies when browsing websites? 
Well, this is the this is the thing,   isn't it? This is a great example of how 
often it is a question of convenience.
13:49 Sometimes when you go on the internet, 
you know, I don't know if it's the same   everywhere in the world, but certainly in 
Europe, where I live, you have to, you know,  
13:60 websites are obliged to make you choose 
to accept or not accept cookies. Cookies,  
14:08 I don't know if this is the same word that's used 
in every language. Cookies are little files, which  
14:14 are saved on your computer through your internet 
browser, and they kind of work in two ways.
14:21 So the file helps you as a user of a website, 
for example, to it saves certain information  
14:29 about the way you've browsed on that website, 
which can be convenient, it can help you to it  
14:35 remembers certain preferences that you might have 
it remembers certain things you've done on that   website, so that you can easily, for example, stay 
signed in, you know, whenever you go to a website,  
14:47 you sign in, you can stay signed in, because the 
cookie remembers who you are, remembers what your   information was. So you don't need to sign in 
again every time. But also, on the other hand,  
14:57 it allows that website or that service to remember 
to gain certain bits of information about you too.
15:04 I'm not entirely sure what that 
information is, it could be limited to,  
15:10 for example, your browsing, your browsing 
preferences, what you've done, what you've   clicked on and what you've done. For example, 
if you go on a shopping website, and you look  
15:19 at certain products, the cookies will help that 
website remember the things that you looked at,  
15:25 which presumably it will then use to try and send 
you targeted advertising. But there could be a lot  
15:32 more, it could be they could be gathering a lot 
more information and saving it than just that.
15:39 So that's basically what cookies do. Now,   personally, I don't really want all of these 
websites to remember everything about me,  
15:46 and to have access to all of my information. 
When I go to a website for the first time,  
15:52 I'm given the option to accept cookies or 
reject them or manage my cookie preferences.
15:58 And when you actually manage them, you 
realise the amount of information you're   giving away to a website. But the problem 
is that, of course, you go to a website,  
16:07 you just want to do something quickly. And so 
naturally, most of the time, I expect most of  
16:13 us click accept all just to get that thing out 
of the way so that we can then start using the  
16:18 website to do whatever we were doing, right? I 
do try to block as many of the cookies as I can.
16:29 But it's annoying, because sometimes you have 
to go in and manage and uncheck various boxes,   select different things before you can 
continue. It's rare that websites just  
16:38 give you the option to reject all cookies. But 
if I am given that option, that is what I do.
16:43 But it's often really annoying and 
inconvenient. And a lot of the time,   I'll just whatever accept all not really wanting 
to, but I'll do it. But that's just a small  
16:53 example of how we do have to make this trade 
off between, you know, privacy and convenience.
16:59 What about you? Do you always accept cookies 
when browsing websites? Or do you you know,  
17:05 choose the less convenient route of having to 
manage your preferences? Do you always read  
17:12 the terms and conditions before clicking I 
accept? I mean, I'd be surprised if anyone  
17:20 really read all of those terms and conditions, 
like whenever you sign up to a new service,   there are pages and pages of legal terms and 
conditions. Normally, we just scroll through  
17:31 them, don't we? And just like whatever, 
accept all. Yeah, I'm sure it's fine.
17:36 But we don't know really what we're signing up 
for. And are there any apps or services which you  
17:43 don't use or don't like because you're concerned 
about how they use your data? Loads of them,   I mean, you know, all sorts of services. 
I'm concerned about how all of them do.
17:54 But of course, there are controversial issues 
relating to this, even political issues.   Like for example, you know, we think about the 
app TikTok, and there have been all sorts of  
18:04 things about that. And it's quite controversial 
because if I talk about this on my podcast,  
18:10 I've got lots of listeners in different 
countries where I don't know, you might  
18:15 feel it's unfair for me to talk about TikTok as a 
website, as an app that I don't want on my phone  
18:23 because of the way that it's going to collect 
my data and where that data is going to end up,  
18:29 who's going to get that information and who's 
going to benefit from it when pretty much any app,  
18:36 any social media app that I have on 
my phone will be doing the same thing,   collecting my data and using it not just to 
advertise to me, but for whatever reason.
18:46 And we're talking now about kind of global, even 
military purposes one day. I mean, it's insane,  
18:55 really, when you think about what what 
companies and possibly governments can  
19:02 do when they have access to your device, if you 
are, if you allow them to, if you allow them to,  
19:10 they could access not just all the information 
saved on it, the stuff that you've websites you've  
19:16 looked at, but also all your emails, all your 
messages and the microphone in your camera and  
19:23 the microphone in your phone and the 
camera in your phone. That is possible. It's definitely possible. I don't know 
to what extent it's actually done, but  
19:33 it certainly is possible that if you can imagine 
a scenario where we're in a war, a global war,  
19:42 and one one side and the other side, you know, 
they're finding every way that they possibly  
19:48 can to win the war. A lot of the time that is 
a question of gathering information in any way  
19:55 that they possibly can for their advantage, 
if they can access somehow, if they had some  
20:01 access point to your device in your home, in your 
hand, that could be a huge strategic advantage.
20:10 Not only that, there's also the fact that if you 
are using a certain app on your phone regularly,  
20:18 that could be a source of sort of 
information control as well. I mean,   it's huge. And I'm not saying 
this one side or the other.
20:25 I'm sure that every successful social media 
app has this potential within it. So, you know,  
20:35 all apps, really, I'm concerned about. It's 
hard to judge, you know, between them all.
20:44 It's terrifying even to think about these things, 
but I think that this is a realistic concern,  
20:49 which is why some countries have banned 
certain social media apps, you know, because  
20:57 those because it becomes a kind of a potential 
threat for various reasons. Hello, I just want  
21:05 to come in here and add the first of several 
interruptions that I'm going to make during   this episode. This one right now is going to be 
about a case study of TikTok and data concerns.
21:17 And I wanted to just sort of expand on this. 
First of all, I do want to be fair and say  
21:23 that I'm just using TikTok as an example here 
because it has been extensively written about and  
21:29 a significant number of people have expressed 
these concerns so much so that it's perhaps  
21:35 the primary example of a phone app, which causes 
people to worry about their data. But of course,  
21:41 this is largely in the West, of course, 
because the app is associated with China.
21:47 And you could probably say the same thing about 
other similar apps, which are based in the USA,  
21:52 for example, from a different perspective, those 
apps will, you know, collect a similar amount of  
21:57 data in a similarly intrusive way. So to an 
extent, they are all they're all doing it,  
22:03 they're all at it. But here is what people 
have said about TikTok just as a case in point.
22:09 And later in this episode, I will give a 
similar case study about Facebook or meta,   which is, of course, based in the USA. 
So I got the following information about  
22:18 TikTok by asking Google Gemini, which is, you 
know, it's essentially a kind of AI powered  
22:25 search tool for Google. And it based its response 
on a variety of sources, including various media  
22:33 outlets, government agencies, VPN services, and 
universities, mostly USA based, I should add.
22:40 Anyway, here's what I found. So concerns about 
TikTok and personal data, primarily revolve  
22:47 around the vast amount of information the app 
collects, and the potential for this data to be   accessed or misused, particularly by the Chinese 
government due to TikTok's parent company,  
22:58 ByteDance, being based in China. So 
here's a summary of the key concerns. First of all, extensive data collection, 
TikTok collects a wide range of user data,  
23:08 often beyond what many users might 
expect. While many apps collect user data,  
23:13 some aspects of TikTok's practises have 
consistently raised more alarms due to  
23:19 their nature and the company's Chinese 
ownership. Is that debatable? Is that a   debatable point? Here are some of the 
most shocking or concerning aspects.
23:28 So extensive biometric data collection, 
TikTok has acknowledged collecting biometric  
23:36 identifiers in regions where were 
permitted by local laws. So they  
23:41 haven't apparently haven't broken laws in 
doing this. This can include information   about facial features, body features and 
voice prints, right? Facial features.
23:52 So for example, what your face looks like, body 
features, even being able to identify maybe your,  
23:57 your your outline, or other other parts 
of your body. I don't know what those body  
24:02 features could be, but more than just your face 
and voice prints. So what your voice sounds like,  
24:09 that's a bit much, isn't it? So they can 
identify you by your voice or by your face.
24:14 The concern here is not just the 
collection itself, but the potential   for this highly sensitive permanent data 
to be used for identification, tracking,  
24:23 or even manipulation, especially if it were 
to fall into the wrong hands or be compelled  
24:29 by a government. You can imagine how that 
kind of information could be used in less,  
24:37 let's say, entertaining ways. Also, keystroke 
patterns, that's basically recordings of how  
24:44 you have actually typed on your keyboard or 
where your finger has gone on your screen.
24:50 So keystroke patterns and rhythms, battery state 
and audio settings. Beyond typical app usage data,  
24:57 reports have indicated TikTok collecting highly 
granular device information. Granular means  
25:04 in very small details, such as keystroke patterns 
or rhythms, battery state and audio settings.
25:10 This level of detail about how you interact with 
your device and its physical state goes beyond   what many users would expect or consider necessary 
for a short form video app and could potentially  
25:21 be used for advanced device fingerprinting 
or behavioural analysis. Clipboard access,  
25:30 there have been so your clipboard is any 
time you copy something on your phone,   let's say some text or a photo, you copy it in 
order to then paste it somewhere else. It goes  
25:40 onto your clipboard, right? So clipboard 
access, there have been numerous reports,  
25:46 particularly concerning iOS devices, 
where TikTok was found to be frequently   accessing users clipboard content, even 
when the app was in the background.
25:56 So even when the app is, you're not even looking 
at it, you've just you've moved on to something   else, you're in your emails or whatever, and 
you're copy pasting some text or a picture.  
26:04 TikTok is still recording what you're adding 
to your clipboard, apparently. While TikTok  
26:09 has claimed this was for anti-spam and 
legitimate features, the automatic and  
26:14 frequent nature of this access without explicit 
user consent was a significant privacy breach.
26:21 This could potentially expose sensitive 
information copied by users, such as passwords,  
26:27 financial details, or private messages. Anything 
you've copied onto your clipboard, your password,  
26:34 yeah, bank details, anything, even when you're in 
another app doing something completely different.  
26:39 Also, metadata of content created, even unshared 
content, TikTok reportedly collects metadata.
26:47 This is, I suppose, information about 
the videos or audio or pictures on your  
26:55 phone. Right, that could include, I 
don't know, facial recognition data,  
27:00 or maybe location based information. 
So it reportedly collects metadata  
27:06 about the videos and images users create, 
including identifying objects and scenery.
27:11 So it can identify objects and scenery. 
It could maybe locate the picture based  
27:18 on the content of the picture, facial and body 
features, again, voice prints and spoken words,  
27:24 even if the content is never actually posted or 
shared. So if it's just saved in your device,   this means that the app is analysing and 
potentially storing information about you,  
27:33 about your personal creations before 
you even decide to make them public,   raising questions about the scope 
of its internal surveillance.
27:42 I should say also that those of you listening 
to this, you feel it's unfair that I'm talking   specifically about TikTok, although I have 
mentioned that and I will talk about Facebook  
27:51 and meta in a moment. I should say everyone 
should be worried about this kind of intrusion,   right? This is not just a kind of like one 
country versus another country type of thing,  
27:60 right? I think we're all subject to this level 
of surveillance and this level of intrusive  
28:06 data harvesting of our personal information. I 
think it's completely unacceptable for anyone,  
28:11 wherever you are, whoever you are, right? 
In whatever political context you live in.
28:16 I think that we can all agree that this is, this 
is surely a breach of our privacy. And then well,  
28:24 it's the, the response from Gemini does 
mention Chinese government compulsion and  
28:31 the potential for that. This is the overarching 
and most alarming concern due to bite dances.
28:37 That's the TikTok's parent company headquarters 
in China. And the, I think the, the suggestion is  
28:43 that because of the reach that Chinese, that 
the Chinese government has in China relating  
28:49 to being able to involve itself in the affairs 
of any company that exists and operates there,  
28:58 you know, because of bite dances, headquarters 
in China, there are legal frameworks in China   that could compel the company to share data 
with the Chinese government for intelligence  
29:06 or national security purposes. Despite TikTok's 
assurances of data localisation, storing US data,  
29:13 US user data in the US, for example, the 
fear persists that the Chinese government   could still legally demand access to user data 
algorithms, or even exert influence over content  
29:23 moderation and algorithmic recommendations 
to push narratives or suppress dissent.
29:28 This goes beyond typical commercial data 
exploitation and into geopolitical and national  
29:33 security territory. And lack of transparency and 
shady practises, shady practises would be like  
29:43 suspicious things. Historically, TikTok 
has faced criticism and even fines.
29:50 So having to pay money for a lack of transparency 
in its data collection practises, transparency  
29:56 relates to, you know, information or things 
being visible. So not hidden, but transparent,  
30:05 meaning that we can, anyone can see into the 
company and see what it's doing. Transparency is a  
30:12 thing that's encouraged, like the GDPR regulations 
in Europe are supposed to ensure a level of  
30:20 transparency so that ordinary users accessing 
a website or an app or an email newsletter  
30:26 or something will, will know that the company 
involved is, is, you know, going to do this, that  
30:34 or the other with your data or is taking this kind 
of information from you, that it should be open.
30:39 This information should be available 
to everyone. That's what transparency   means. So TikTok has faced criticism and 
fines for its lack of transparency in its  
30:48 data collection practises and how it handles 
user data, particularly concerning minors,  
30:54 that's underage people, children, 
and international data transfers. This perceived ambiguity fuels distrust 
and makes it harder for users to truly  
31:04 understand what they are agreeing to. So even 
when you agree to the terms and conditions,   which you haven't even read, it might even be 
unclear exactly what you're agreeing to anyway,  
31:14 even if you did read them. So these aspects 
combined contribute to the heightened scrutiny.
31:20 That means the sense that people are looking at 
TikTok and its practises in more careful detail,  
31:26 examining it, and the, and concerns surrounding 
TikTok's data collection differentiating it from  
31:34 many other social media platforms. It should 
also be said that while TikTok has faced unique   scrutiny due to its Chinese ownership and 
specific data practises, similar concerns  
31:43 about extensive data collection privacy and even 
potential misuse of data are widespread across  
31:48 many popular Western based apps, particularly 
social media platforms and those that offer free  
31:54 services. And of course, it's been said before 
that when a service offers you something free,  
32:00 that's when you realise that you 
are not necessarily the customer,   you are in fact the product, you are 
the commodity, which is being traded.
32:07 Um, you know, anyway, so there you go. I 
don't mean to make this a political issue   per se. In fact, as I've said, I think that 
largely we are all potentially affected by  
32:18 forms of authoritarianism through 
economic and technological means,  
32:23 regardless of our location 
or political affiliations. We're all subject to these 
kinds of things. In other words,  
32:31 we are all spied on and have our freedoms 
compromised by powerful people. And one of  
32:36 the ways they do it is with a kind of 
economic and technological supremacy. So there you go. Right now, anyway, anyone 
fancy a pint? Right back to the episode.  
32:49 What do you think about this? How would you 
answer that question? Are there any apps or   services which you don't use because you're 
concerned about how they use your data? Uh,  
32:58 we could say the same thing about Facebook. I'm not a fan of Facebook and I generally try to 
avoid going on it because I don't like the way  
33:10 that they've used our data in the past. And there 
was a scandal involving Facebook and a company  
33:16 called Cambridge Analytica, which was a lot, which 
was a basically Facebook allowed certain accounts  
33:27 to be targeted by, um, not just advertising, 
but by political, politically motivated,  
33:36 um, posts. And this is all connected to the 
Brexit referendum that there was kind of shady,  
33:44 um, organisations posting, let's say fake 
news in a very targeted way to try to promote,  
33:54 um, Britain's exit from the European Union. Hello again. So it's already time for my 
second interruption with case study number two,  
34:04 which is the Cambridge Analytica and 
Facebook scandal. I thought it would   be worth going into some detail about 
this because, you know, the more detail,  
34:11 the better, rather than just hearing 
me sort of rambling about this. I thought that I would look into it in a bit more 
detail and give you some more specifics. So, um,  
34:20 this information about the Cambridge Analytica 
and Facebook scandal is from a variety of sources,  
34:26 including UK universities, the UK government 
website, Wikipedia and other international trade  
34:32 services. Also famous investigative journalism 
by Carol Cadwallader and a really interesting  
34:39 couple of Ted talks that she did on this as 
well, which you can see on the Ted website.
34:45 So the Cambridge Analytica and Facebook 
scandal, which came to light in early 2018,  
34:51 involved the unauthorised collection and use of 
personal data from millions of Facebook users  
34:57 for political purposes. What happened? First 
of all, the data collection mechanism in 2014,  
35:05 a Cambridge University researcher named Dr. 
Alexander Kogan developed a personality quiz app,  
35:13 a personality quiz app called This is Your 
Digital Life. Users who downloaded and took  
35:19 the quiz unknowingly gave the app permission 
to access not only their Facebook profiles,  
35:26 but also the profiles of their Facebook friends. At the time, Facebook's API, that's application 
programming interface. I guess that's the way that  
35:38 websites interface with other websites and sort of 
share data across each other and things like that.  
35:45 So Facebook's API allowed developers to 
collect data on users friends by default,  
35:52 meaning that was the normal setting that 
apps that worked with Facebook, you know,  
35:58 sometimes you'd be on Facebook and use, 
I don't know, there'd be like a game app,   like it used to be words with friends was 
that done through Facebook or other games  
36:07 like that where you would essentially allow 
the app to share your Facebook information.
36:14 So their standard default settings for 
sharing data included your friends data,  
36:21 even if the friends themselves hadn't 
consented. So if you accepted to do  
36:26 this quiz through Facebook, all of your 
friends data would be shared with this app,  
36:33 even if your friends hadn't consented. 
Secondly, sale to Cambridge Analytica. Kogan then sold this collected data to 
Cambridge Analytica, that's CA for short,  
36:44 a British political consulting firm. 
CA was backed by conservative donors  
36:49 like Robert Mercer and former Donald 
Trump political adviser, Steve Bannon,   as a co-founder. And they're also sort 
of allegations of other kinds of funding.
37:02 Psychographic profiling and micro targeting 
Cambridge Analytica used this vast,  
37:08 this vast dataset to create psychographic 
profiles of voters. These profiles aimed to  
37:16 identify individuals personality traits, 
vulnerabilities, that's like their weaknesses,  
37:21 maybe issues, topics that they're particularly 
sensitive about, and political leanings. The  
37:27 alleged purpose, when we talk, when we use 
the word alleged, it basically means that  
37:33 this is what people have said, it might not 
have been fully proven in a court of law. So you saying alleged or the allegation 
is a way of kind of saying that this isn't  
37:46 necessarily true, but this is what people 
have said. Right? So the alleged purpose   was then to use this information to develop 
highly targeted political advertisements and  
37:56 messages designed to influence voters 
behaviour in elections, including the  
38:01 2016 US presidential election and the Brexit 
referendum. Right? So all this information  
38:08 harvested shared with Cambridge Analytica, 
and their aim allegedly was to target voters.
38:16 What I understood is that they targeted voters who 
were undecided. So let's say if it's the Brexit  
38:22 referendum, you've got voters who clearly are 
going to vote remain, voters who clearly are going  
38:28 to vote leave. And then there's the people in the 
middle who haven't quite made up their mind yet.
38:34 These are the people who were targeted, right? 
The company claimed to have up to 5,000 data  
38:40 points on every US voter. Point four, scope of the 
data. So the size of the data set while Cambridge  
38:49 Analytica initially claimed to have collected 
data from about 30 million Facebook users,  
38:55 Facebook later confirmed that the data 
of potentially over 87 million users was  
38:60 improperly accessed with a significant 
majority being from the United States. So the alleged scandal, the core of the scandal 
was the unauthorised harvesting. That's collection  
39:12 without authorisation, the unauthorised harvesting 
and exploitation that's use for some reason of  
39:19 personal data for political manipulation. That's 
really serious and seriously undemocratic.
39:26 Key allegations included lack of informed consent. 
There's that word consent, meaning giving your  
39:34 consent, saying that you agree, that you allow 
something to happen. Millions of Facebook users  
39:39 had their data collected without their explicit 
knowledge or consent for this specific purpose.
39:45 While some users consented to the app for a 
quiz, they were unaware their friends data   would also be taken or that the data would be 
sold and used for political targeting deception.  
39:56 The app developer Alexander Kogan allegedly 
deceived users about how their data would be  
40:02 used and Facebook's culpability. Facebook was 
heavily criticised for allowing its platform's  
40:08 API to enable such broad data collection 
from friends of users, also not adequately  
40:15 monitoring how third party developers were using 
the data they collected, being aware of Cambridge  
40:20 Analytica's improper data acquisition as 
early as 2015, but failing to ensure the  
40:27 data was destroyed and not acting decisively 
enough until the story broke publicly in 2018.
40:33 So apparently they knew about this in 2015 
Facebook. They knew that Cambridge Analytica  
40:39 was doing improper things, but they didn't do 
anything about it until it became public. And then   they realised, oh, we've got to do something about 
this because it, you know, it makes us look bad.
40:49 Initially downplaying the incident as not a 
data breach because no systems were hacked,  
40:54 but rather a violation of its terms of service. 
Critics argued this was a semantic distinction  
41:01 that obscured the significant policy 
violation. So a semantic distinction.
41:06 When we talk about semantics, we just talk 
about the meanings of words, like this is what   semantics is. So this is a typical sort of legal 
response where the argument is over, you know,  
41:19 the precise definition of the words used. So 
they were saying, this was not a data breach.
41:24 No one was hacked. It was a violation of our terms 
of service. And the critics said that this was,  
41:31 this was just a way of avoiding the real 
issue, which was the privacy violation.
41:37 And then we have the manipulation of elections,   the most serious allegation was that this 
data was used to create sophisticated,  
41:44 manipulative political campaigns designed 
to influence the outcome of major elections,  
41:49 raising concerns about the integrity of democratic 
processes. The scandal led to significant public  
41:55 outcry, governmental investigations, calls for 
stricter data privacy regulations like GDPR,  
42:03 and a major hit to Facebook's reputation. Yeah, 
this is definitely the time when I really was  
42:09 turned off Facebook and I saw it for what it is, 
which was, I just felt like it was a cynical,  
42:19 a cynical site just gathering my data 
and then potentially manipulating me.
42:25 And I just, it made me feel unwell. Not only 
that, but also just the experience of using  
42:32 Facebook these days, I find it bewildering 
and unnecessarily complex and almost you  
42:38 can feel like this heavy presence of this 
algorithm pushing certain accounts towards   me and pushing in lots of other content that 
I really didn't choose to look at. That was  
42:49 the idea with Facebook originally is that you 
would choose exactly who you followed and stuff.
42:54 And the bewildering number of options 
and privacy settings, which is almost,  
42:60 you feel like it feels like it's intentionally 
confusing so that you don't really understand  
43:06 how to control it all. And as a result, you kind 
of let it do what it wants to do, which is push  
43:12 certain things at you. And you know, half my 
friends on Facebook seem to be invisible to me.
43:17 I don't know why. Anyway, 
continuing this case study,  
43:23 Facebook faced numerous fines to pay money 
and had to implement changes to its data  
43:29 privacy policies and API access. 
You have to watch these companies.
43:34 Their main agenda, remember, is to 
make money and we are their commodity.   They don't necessarily have 
our best interests at heart,  
43:42 despite the way they present themselves as 
being this kind of friend based exploit,  
43:50 which is all there to help connect people. 
That's all just part of their marketing. What they really want to do is use our attention 
to grow their profits and their influence.  
44:03 So we have to remember that and be 
careful of what we allow these companies,  
44:09 these huge, most, these are the most powerful 
organisations in the world. So we have to,  
44:14 you know, keep them in check and 
see what they're actually doing. Right. That's the end of case study number two,  
44:20 back to the episode. So some relevant vocab, we 
have privacy or privacy in USA data security,  
44:31 personal data, all the information 
about ourselves, a trade off.
44:38 Right. We have to trade these things 
off. You have to make a trade off between  
44:43 personal privacy and convenience 
compromise in this arrangement. We accept a certain level of compromise. 
That's where you accept perhaps less.  
44:53 You give something away in 
order to get something you want. This is the nature of a trade 
off. In order to get something,  
45:02 you have to give something away. 
You have to make compromises. Right. Intrusive is an adjective 
that we could use to describe  
45:11 certain apps or services that go into 
your personal data and gather things,  
45:18 information about you. This is quite 
intrusive to intrude on your privacy.
45:24 Right. An intruder, an intruder would be a 
person who comes into your private space,  
45:30 perhaps to steal something, a robber, maybe that 
could be an intruder. Intruders are intrusive.
45:37 You could say also that, you know, these 
apps and things are quite intrusive. They   go into our personal areas, into our 
personal zone and take information  
45:46 about us. Or at least we kind 
of give away the information   without even realising it because we're not 
really paying attention to all of that stuff.
45:54 Surveillance is another word. Surveillance 
relates to being watched, being surveyed, being  
46:01 essentially spied upon for commercial purposes 
or maybe for political purposes. We're not sure.
46:09 Anonymity. Anonymity is what we probably should 
have on the internet. That's where we're able to  
46:17 browse the internet without the internet itself 
or the organisations on it, knowing who we are.
46:24 You know, like for example, if 
you, you know, you might go on a,   on a, on a forum or a chat room or 
something and choose to hide your  
46:32 identity. In that case, you would choose 
anonymity. You'd choose to be anonymous. Examples of that language. There's always a trade 
off between privacy and convenience when you use  
46:42 social media. Some apps can feel quite intrusive 
because they constantly track your personal data.
46:48 Many people are willing to sacrifice 
anonymity for convenience, but they   may regret it later. What about 
you? What do you think about those  
46:56 things? Let's move on to the second 
point. There are 14 of these points. So let's keep moving. So data sharing, every time 
we sign up for a new account, fill in an online  
47:07 form or even browse a website, we share personal 
data, knowing when, how, and with whom to share  
47:14 this data is crucial. The stakes are high because 
if this information is mishandled through leaks  
47:22 or deliberate misuse, it could lead to fraud, 
spam or loss of control over our digital lives.
47:29 Understanding the fine print and managing our data 
sharing preferences helps us take control. Now,  
47:36 actually the questions, the discussion questions 
for this are the same as in the previous topic. So  
47:41 no need to answer them again, but nevertheless, 
this is quite interesting data sharing.
47:47 Again, this refers to the compromises 
that we might make when we sign up   to various services that we get the 
convenience, but we have to hand over,  
47:55 you know, a lot of our personal information. Bits 
of vocab. The stakes are high because this, this,  
48:03 if this information is mishandled, it could 
be serious, right? So the stakes are high.
48:11 This is quite a nice good 
expression. I'm just going   to note this one down. So the stakes are high.
48:17 It just means that the potential is very high. 
The potential for both good and bad outcomes  
48:25 is very high. Right, so if you're dealing with 
a situation, I guess this comes from gambling.
48:32 If you've put a lot of money into a card game,   right, if you've bet a lot of money 
in a card game, then the stakes are  
48:41 high. The potential for great wins and great 
losses is very high. So it's quite serious.
48:49 If the stakes are low, then it's not that 
serious because you're not really going to   lose very much. You're not really going 
to win very much. It's not that serious.
48:57 So the stakes are high in this kind 
of situation because there is actually   quite a high potential for the misuse or 
mishandling of your personal information  
49:08 in a variety of ways. I mentioned some 
of the more paranoid ways a bit earlier.  
49:14 But we've all we've got other things 
that could be done with your data. First of all, there's simply spam, right, 
which we all know about. This is just the  
49:24 an annoying aspect of modern life that, you 
know, thankfully most of us now have email  
49:29 filters that filter out the spam. And 
if you ever look into your spam folder,   you can see that your internet account is probably 
filtering out dozens of spam emails every hour.
49:42 You know, there's just so much spam from 
companies trying to advertise to us,  
49:47 people trying to steal our information. It 
seems that our personal data is like this  
49:53 extremely valuable commodity these days. 
That everyone is desperate to get hold of.
50:00 We really do live in the data age, I guess, 
because it's our data that everybody wants,  
50:07 that everyone wants to buy and sell for whatever 
reason, the more data these organisations have,  
50:14 the more powerful they are. But anyway, so the 
states are giving away our data could lead to  
50:22 us getting a lot of spam, which is just annoying. 
It could lead to fraud, which is a kind of crime.
50:28 Maybe this could be things like identity, 
theft, or other forms of information based  
50:33 crime. Fraud can be all sorts of different 
things. It could be essentially, you know,  
50:41 lying about information in order 
to get some sort of advantage. It could just mean taking your 
bank details and finding ways to  
50:49 steal your money or loss of control over our 
digital lives. So this would be, for example,  
50:56 losing access to our social media accounts, 
losing our, you know, identity theft, people  
51:05 essentially taking our identities and using 
them. And you know, you think about scammers.
51:11 These are people or organisations who make 
it their work, their criminals, they make it  
51:16 their work to try to find ways to get into your, 
maybe your bank account, find ways to get money  
51:22 from you by tricking you by scamming you. So 
the stakes are high in this situation because,  
51:31 you know, the potential problems that could be 
caused by the misuse of our data can be quite  
51:39 serious. Information can be mishandled, meaning, 
yeah, if it's mishandled, if it's not kept  
51:50 private, if it's not kept secure, if it's lost 
or maybe sold, it could be lost through leaks.
51:58 You know, an information leak is where 
information gets out. Originally,   a leak would be when maybe water comes out of a 
pipe in your home. If you've got a water leak,  
52:10 it means maybe there's a hole in 
one of the pipes in your house and   water is coming out of the hole and 
it's dripping through the ceiling.
52:17 Oh no, we've got a leak. But these days, we 
often talk about information leaks. For example,  
52:23 government, very sensitive government 
information being given accidentally  
52:28 or maybe on purpose to a newspaper or to 
another government that is an information leak  
52:35 or personal information can be leaked. You know, companies like your email provider 
have a company like that would have millions  
52:49 of recordings of people's data and that 
data might be leaked for various reasons.  
52:55 Every now and then you get an email from some 
company saying we've had a data breach or a   data leak means that you've got to change 
your password because blah, blah, blah.  
53:05 Understanding the fine print and managing our 
data sharing preferences helps us take control.
53:11 So they're the same questions as before, 
but we've got words like data breach or   leak third parties. So if you do a, 
if you sign up to Facebook and you  
53:23 think it's just between me and Facebook, 
well, Facebook might actually share your   information with a third party. This is 
another organisation, another company.
53:33 So that's what we mean when we say third 
parties, other companies secondary to the  
53:39 ones that we initially are dealing 
with consent. Consent is basically  
53:44 giving your agreement for certain things to 
happen. For example, we talk about cookie consent.
53:51 Do you consent to cookies from this 
website? Basically, do you say,   yes, I agree. Yes, that's okay. 
That means giving your consent.
54:00 If you consent for something to happen 
means you agree and accept. You said,   yes, you allow it to happen. Terms and conditions.
54:08 I think we understand this is like when you sign 
up for a different website, you have to read the  
54:14 terms and conditions, which essentially explain 
what that website can do with your data. You can  
54:22 opt out. For example, opting out of cookies is 
opting, meaning choosing as an option to be out.
54:30 So essentially rejecting cookies. GDPR, this is 
the sort of regulation procedure in Europe. Which  
54:41 means that companies have to be transparent and 
clear about the way that they are using your data.
54:47 And it's GDPR, which has forced a lot of websites 
to give you the option to opt out of cookies or  
54:55 at least control your preferences with the way 
that cookies are being used and so on. You know,  
55:02 email lists, if a company puts 
your email into an email list,   it should ask for your consent. 
You should be accepting that first.
55:12 But I mean, you know, the number of times we 
end up in email lists without our consent is,  
55:18 it's not good. That's why there is a regulation 
procedure like GDPR in place. Do you have a  
55:25 similar thing where you are in the world? Examples 
again, I never read the terms and conditions.
55:31 So I probably give consent to a lot of 
things without realising it. After the   recent data breach, I decided to opt out of 
sharing my data with third parties. Do you  
55:41 always accept cookies without thinking about 
the implications? Okay, let's move away from  
55:48 sharing our data and stuff and talk about the 
third item in my list, which is digital detox.
55:54 So with our lives increasingly tied 
to screens, whether it's for work,  
55:60 socialising or entertainment, it's easy to 
lose track of time and feel overwhelmed. A  
56:06 digital detox means consciously stepping back from 
technology to recharge mentally and emotionally.  
56:13 It can help us sleep better, reduce stress and 
reconnect with offline hobbies and relationships.
56:20 But actually switching off is 
often harder than it sounds.   So this is digital detox. Normally when we 
talk about detox, we're talking about changes  
56:30 to our diet, to try and eat more healthily 
and to get rid of toxins in our bodies.
56:39 So, you know, I don't know, after Christmas, for 
example, when you eat lots of unhealthy food,  
56:45 that's my alarm to remind me to 
drink water. After after Christmas,  
56:51 yes, you might have a detox in January where 
you try to eat much more healthily and avoid   alcohol and stuff to give your body a 
chance to get rid of all those toxins.  
57:01 A digital detox is the same kind of thing, 
but you essentially get, you give yourself   a chance to get away from all of the digital, 
what would you call it, input in your life,  
57:15 essentially trying to use your phone less, 
going away from screens as much as possible.
57:20 So yeah, with our lives increasingly tied 
to screens, our lives are connected to   screens. We sit in front of screens 
all day at work. And then at home,  
57:28 we're in front of screens for 
our entertainment and stuff. We are connected to the screens. We're tied to 
them increasingly more and more, whether it's  
57:37 for work, socialising or entertainment, 
it's easy to lose track of time and feel   overwhelmed. If you lose track of time, it means 
you get, you lose that sense of what time it is.
57:46 You feel overwhelmed when things feel 
like they are too much. A digital detox  
57:52 means consciously stepping back from 
technology to recharge mentally and   emotionally. Because when we are always 
on our phones, looking at the screens,  
58:02 looking at whatever it is, whatever bit of 
social media we're looking at, of course,   our attention is being manipulated all the 
time where there are lots of tricks designed to  
58:15 make us either keep our attention here, move our 
attention there, and also manipulate our emotions.
58:22 It can actually be quite a sort of manipulative 
experience. Again, without us really realising it,  
58:29 we're constantly being manipulated by the things 
we're looking at, in the sense that we're being,  
58:35 our emotions are being triggered, our 
feelings are being worked on. And when  
58:40 you consider how much sort of information 
you give to the apps that you're looking at,  
58:47 it's quite clever the way that they 
are able to work on your emotions,  
58:53 your preferences, in order 
to keep you on that platform.
58:59 In fact, our attention is, I talked about our 
data being a commodity which is bought and sold.  
59:05 But also largely our attention is a commodity now, 
which these companies trade in. The more they can  
59:15 keep our attention, keep us looking at the things 
they want us to look at, the more value they get.
59:22 And so our attention is also traded 
around. So it's quite, it can be quite a,  
59:30 what's the word for it? It can be quite an 
impactful thing to be on online all the time,  
59:38 not just in a physical sense of the way that, you 
know, the light shining in your eyes, you know,  
59:46 that side of things, but also in a mental health 
sense that constantly being subtly manipulated,  
59:53 being rewarded with dopamine and all these other 
little micro manipulations that are constantly  
59:59 going on that does have an impact on us. So 
stepping away from technology and going back  
60:05 to basics and just living without it for a while 
can be, I think, probably very, very good for us.
60:10 So questions, how much screen time 
do you have each day? Is it too much   or is it okay? How do you know? I mean, 
obviously too much screen time. I think,  
60:23 I think for most of us, it's probably too 
much. It does make me wonder about younger  
60:28 people who have grown up with phones and 
for them, I wonder if they even realise  
60:39 how much screen time they're getting and what the, 
what it's really like to live without screen time.
60:45 What the difference is a lot of people are 
growing up now with no sense of objectivity  
60:51 on this. And I'm showing my age when 
I say this, but I remember, you know,  
60:59 a time when there were no smartphones,   of course. And I remember a time when there was 
no, when there were no mobile phones at all.
61:08 And it's quite a different time, quite 
a different experience. So anyway,   how much screen time do you have? Probably 
too much. First of all, at work, you know,  
61:16 when I'm working, I sit in front of a computer 
all the time, either preparing podcast episodes  
61:21 or doing all the other different things I 
have to do in relation to making podcasts.
61:27 A lot of it does involve sitting here in front 
of a computer. And then in the evening or in   my free time, yeah, of course, I'm often 
on my phone or in front of my computer,  
61:38 probably on YouTube or maybe Netflix or some other 
streaming platform. And yeah, it's, it's too much.
61:45 I think it is too much. How do I know? Well, 
sometimes I just feel a bit burnt out from all  
61:53 of that screen time. Other times I actually get 
a headache from looking at a screen too much.
62:00 And sometimes I wonder if maybe my 
mood is being affected. It's very   hard to know because you've got nothing to compare  
62:06 it against. But it does make me wonder 
if maybe I would be perhaps happier.
62:13 I would might feel more stable, more grounded, 
more mentally well, if I wasn't looking at  
62:19 screens so much. Next question. Are you good 
at detoxing from technology? How do you do it?  
62:26 How could you improve in this area? I think I'm 
fairly good at it because I'm kind of aware of it.
62:32 And there are times when I make a concert, a 
conscious decision not to look at my phone. And  
62:38 I've spoken about about this on the podcast 
before. I find that when I do that, when I,  
62:43 for example, in the evening, actively choose not 
to be on my phone and instead to read a book,  
62:53 or play music, or even just lie down with my eyes 
closed, or go out and socialise with my friends,  
63:03 something like that, I generally 
have a much, much better time. Often, I feel like I'm scrolling my 
life away. If I'm just, it's automatic,  
63:13 the kids are in bed. My wife and I have had 
dinner, we end up on the sofa on our devices.
63:19 And often that is time I think that's 
wasted. I'm just scrolling through YouTube,   searching for a video that's just 
going to get my attention. I'm not  
63:27 necessarily looking for something in particular. I'm just letting YouTube choose for me, 
just waiting to find the thing that's  
63:34 going to catch my attention. And it's 
a sort of mindless state to be in. I do  
63:42 feel generally happier when I put the phone 
down, get a book and just read from the page.
63:50 It's a much more wholesome feeling. And I think I   probably get better sleep as a result. 
I think I could do that a lot more.
64:00 And I often dream about doing that a lot 
more. It's terrible, isn't it really? I   think we are compulsive with our phones. 
We just automatically reach for our phones,  
64:11 because perhaps we are caught up in a cycle of 
reward, you know, being rewarded with little,  
64:18 it's quite addictive, you know, using 
your phone, it's quite addictive. The reason it's addictive is because the people 
who make these devices and the software on them,  
64:27 they know exactly what they're doing. They know 
exactly how to keep you engaged and how to reward  
64:34 you with little satisfying hits of dopamine. 
That's just the way that these systems work.
64:42 They hold your attention. They never let you get 
bored because there's always some new little bit  
64:48 of stimulation going on. And you see this 
more and more with like short form content,   which is designed to make sure you 
never get bored because something  
64:58 new is only a swipe away and other little tricks. Like for example, I, you know, I 
publish on YouTube and I'm, I use the  
65:06 YouTube studio in the YouTube studio. There's 
the creator dashboard. And in that dashboard,  
65:12 you actually have a little leaderboard of your 
10 most recent episodes that you've uploaded.
65:19 And if your most recent episode is number 
one in the list, meaning it's getting more   views than the other previous 10, that's 
what you want. That's definitely what  
65:29 you're always looking for. And your most 
recent episode is somewhere in that list,   that, that premier, premier league of 
episodes, let's say, of your last episodes.
65:40 When your episode hits that number one spot, 
it actually rewards you with a little shower   of confetti. So a little shower of confetti 
comes out from the number, it goes one out  
65:50 of 10 little shower of confetti. That little 
shower of confetti does definitely trigger  
65:57 some sort of little reward mechanism within you 
where your brain makes you feel a little happy.
66:03 It's a little response to that shower of confetti,   which it's essentially giving you dopamine. We 
know what we know this we've researched this,  
66:13 we know what's going on. It's a 
little dopamine hit that you get. It's the same kind of hit that you get when 
you see that someone has liked your video or  
66:21 given you some positive response to a post that 
you've written little number, a little, a little,  
66:28 you know, yeah. What's the word 
for it? Confetti. That's it.
66:33 A little confetti explosion. 
And as a result, you know,   it's quite addictive. You end up looking for that.
66:40 You're always searching for that little 
feeling without even realising it's not,   it's not necessarily conscious for you, but 
it's just something you're always reaching  
66:49 for your phone, looking for that little 
moment of stimulation. I think that I  
66:55 could probably detox from technology a lot 
more. It would probably be better for me.
67:00 I guess some relevant vocab fit for 
this screen time. We know to unplug. So  
67:08 you plug in your phone. If your battery is running out, if your 
battery is dying, you need to plug it in,  
67:16 right? Plug in my could you plug in my phone 
and to disconnect would be to unplug. So that's  
67:22 literally plug in and unplug, but also you can 
unplug yourself from technology. Mindfulness.
67:29 This is just that sense of being aware 
of yourself, aware of what you're doing   and aware of how you're feeling in every 
moment. It's a good way to try to keep  
67:39 yourself grounded so that you don't end up 
in kind of, let's say, unhealthy behaviour.  
67:45 If you're always staying attentive to 
what you're doing and how it's feeling.
67:52 FOMO. Have you heard of this? Fear of missing 
out. This is one of those perhaps things that   keeps us engaged with our technology, with our 
apps, with our social media, because we've got  
68:03 this sense that we don't want to feel that 
we're missing out on stuff that's happening. This is why we're often scrolling through 
different forms of social media, because we are  
68:12 aware that there is a whole world of stuff going 
on that we feel we can't miss out on. This is the  
68:19 fear of missing out. It's a thing that drives 
us to stay connected and then self-discipline.
68:26 This is just having the self control to, for 
example, disconnect, unplug and not worry about  
68:34 all that stuff. For example, I try to unplug from 
all devices after 9pm to reduce my screen time,  
68:41 turning off notifications is a simple 
way to create boundaries. These are   like lines that you would draw to 
create limits for certain things.
68:52 For example, if you want to limit the 
influence that your phone is having,  
68:58 the way it's accessing your personal life, 
you might set a boundary. For example,  
69:07 making sure that you have all push 
notifications turned off on your   phone so that it's not crossing over 
into your personal sphere too much.  
69:16 Practising mindfulness helps me resist FOMO when I 
see everyone posting holiday photos, for example.
69:22 Okay, moving on to number four, oh, 
I needed to drink water. Let's have   a water break then before item four, 
which is all about online behaviour,  
69:30 the way we behave and the way other 
people behave online. Drink a water first. Here we go. I'm wondering if I'm going to 
get through all 14 items in this list. I'm  
69:41 trying to go quite quickly, but I think this 
might have to be a two part episode because  
69:48 it's been, it's getting on for an hour. It's been 45 minutes and I'm only 
on the fourth item out of 14. This  
69:56 might be a double episode. You 
can let me know what you think. So number four, though, is online behaviour.  
70:03 The internet gives us an enormous platform 
to express ourselves, but it also means we  
70:08 have a responsibility to behave ethically 
from spreading kindness to avoiding harmful  
70:15 activities like trolling or cyber bullying. Our 
digital actions have real world consequences.
70:22 How we conduct ourselves online not only affects 
others, but also shapes our own reputations.  
70:31 So the internet gives us an enormous 
platform to express ourselves. Yes,   it's a space where we can express our thoughts,  
70:40 feelings, opinions, but it means we have 
a responsibility to behave ethically.
70:45 We have to remember that our actions have 
consequences and that although there is this  
70:54 kind of filter again of the internet, we sort 
of, they say that the internet connects us,  
71:03 brings us all closer together. But to an 
extent, the internet separates us as well,  
71:08 where you could be on your own 
in your room, or I don't know,   sitting on your toilet on social media, you 
read someone's post and you don't feel that  
71:18 connection. It's not as if you are in the same 
room, sharing the same space as that person.
71:24 And as a result, you might write something that 
you would never say to that person face to face.  
71:31 You might do something that would be unthinkable, 
that would be so hurtful and so rude. But because  
71:39 you essentially feel a lot of distance, and 
because you feel like the social conventions are  
71:45 not the same on the internet, and because you feel 
that there is no repo, there are no repercussions.
71:51 Like if you said something rude to 
someone in a room, they might be able to,  
71:57 essentially, they might be able to even touch 
you, hurt you physically, or at least respond   to you or you'd have to live with that person 
in that room at that moment. After having said  
72:08 it, it would be awkward. Whereas on social 
media, you can just kind of like respond   without even thinking and never even 
have to worry about it ever again.
72:18 So to an extent, the internet divides us, 
it separates us and takes away a certain  
72:24 kind of human closeness that perhaps is really an 
important factor in making sure that we are, that  
72:35 we treat each other in an appropriate way and that 
we don't hurt each other. So we always have to  
72:42 remember that we have a responsibility to behave 
ethically and morally from spreading kindness to  
72:51 avoiding harmful activities like trolling. 
I think you probably know what trolling is. It's essentially kind of like, you 
know, being rude to people online,  
73:00 just for the hell of it often. Cyberbullying is 
even worse than trolling. This is where you are  
73:06 essentially harassing or abusing people 
online and it can be immensely hurtful.
73:14 Our digital actions have real world consequences. 
Okay, so some questions. Have you ever seen  
73:20 examples of abusive online behaviour and does 
social media sometimes make you feel bad? Yeah,  
73:29 I mean, I've seen lots of examples of essentially 
people just being really mean and nasty online.
73:38 Twitter or X as it should be called. You see 
so much of that kind of thing where maybe even  
73:46 just someone fairly innocently does something 
online and they just get completely abused for  
73:51 it for whatever reason. And there is 
the there's the phenomenon of social  
73:58 shaming, right? Public shaming, which is where 
yeah, someone does something or said something.
74:06 Maybe they shouldn't have said it 
might have been a bit stupid, but they   just get countless responses from people 
being abusive and nasty, not just criticisms,  
74:17 not just sort of diplomatically worded 
criticisms, but straight up abuse. And,  
74:26 you know, you wonder how that sort of thing 
happens. I suppose it's because there aren't  
74:32 really many other opportunities for this 
sort of thing to occur in the real world.
74:37 It's rare that you can get all so many people 
in a huge crowd, all essentially criticising  
74:46 or abusing one person all at the same time. 
It's almost sort of physically impossible,   unless you're all in a big room together. 
And that one person is standing up on the  
74:54 stage and everyone is shouting abuse 
at them and throwing tomatoes at them.
74:60 But it's quite difficult and rare for that 
physical situation to happen. Whereas online,   it's incredibly easy. So of course, 
I've seen all sorts of things.
75:08 Example, I can't think of any 
specifics now of just, you know,   someone online being abused and it must be 
crushing. It must be horrendous. I mean, okay.
75:18 So maybe I've seen examples of let's say a 
famous online English teacher who arguably was,  
75:32 who made a couple of videos that were 
questionable in the way that they treated  
75:41 non-native speakers, making mistakes. And 
the sort of subtext of the videos was if  
75:51 English is not your first language, then 
you might sound a bit stupid if you say  
75:57 certain things. Or if you speak with this 
accent, it makes you sound uneducated.
76:03 So try to speak with this accent. The 
subtext being that maybe a bit clumsily  
76:10 perpetuating certain, what's the word for 
it? Prejudices against non-native speakers,  
76:19 right? Like native speakerism being perpetuated 
by an English teacher on YouTube. And then another  
76:26 English teacher comes in and publicly criticises 
them on their channel and even suggests that  
76:35 their followers should go and criticise the 
other teacher and make their life a misery.
76:43 That kind of thing is almost encouraged 
by the internet. You know, they call it   YouTube drama. That sort of thing is almost 
encouraged by the social media like YouTube  
76:55 or maybe Instagram or whatever works rather 
than simply the one person directly talking  
77:02 to the other person in private and saying, I 
think the way you're doing this isn't right.
77:07 And criticising them privately, instead sort 
of mobilising an army of people who then  
77:13 criticise the other person for it. You know, 
it's like the internet almost encourages this  
77:19 sort of thing. But in actual fact, it can be 
incredibly upsetting and damaging for the poor  
77:27 person who suddenly is being criticised 
by so many people all at the same time.
77:33 It's incredibly dangerous. Social media does make   me feel bad sometimes. Just when I 
see that sort of thing happening,  
77:42 when I see the kind of the worst of human 
nature being set free, being allowed to happen.
77:50 Yeah, that makes me feel bad. You kind 
of lose faith in humanity when you see  
77:56 abusive comments and things like that. And I saw 
a lot of that stuff back in the days of Brexit,  
78:04 when there were, you know, 
you got a lot of abusive.
78:09 If someone basically came out and 
criticised Brexit, a lot of other people,   it is very kind of combative. And it 
felt like two sides fighting against  
78:20 each other. And there would be 
a lot of abuse thrown around. And you just think this is not the right way 
to have a normal, mature conversation about a  
78:29 serious subject. It became just a way to throw 
abuse at each other. Yeah, so there you go.
78:38 Again, I think somehow the mechanics of social 
media allow the worst of human nature to come  
78:43 out. It's almost as if, as humans, we've never 
before in our evolution or in our history had this  
78:52 kind of social dynamic. And as a result, we kind 
of lost our ability to moderate our behaviour.
79:01 And the result of that is that people 
end up feeling absolutely terrible  
79:08 in their online interactions. I think 
there have been cases of, obviously,   there have been many cases of serious mental 
health issues as a result, but also suicides and  
79:18 other things like that. I think it's a dangerous 
thing if we if we're not mindful about it.
79:23 So relevant vocab here is cyber bullying. Bullying   normally happens in school. It's 
like kids bullying other kids.
79:33 Like you get a bully who picks on other 
children, either physically abusing them   or mentally abusing them, harassing them. And it 
can be horrible. The kids can be very depressed.
79:44 They don't want to go to school. 
It can affect their schooling. They   can have serious mental health 
issues as a result of bullying.
79:52 Cyber bullying is more or less the 
same thing, but on the internet,   right? Trolling, we know about hate 
speech. This is abusive language,  
80:02 language used in a very hateful way. It could have 
a racial element to it or a sexual element to it.
80:08 Harassment is similar to bullying. 
Digital footprint. Your digital footprint  
80:13 is essentially the sort of trace that you 
leave in terms of your data usage online.
80:20 So you leave a kind of trail behind you 
whenever you use the internet. This is  
80:26 your digital footprint. We need to 
be aware of our digital footprint. People can, I don't know how it relates 
really to your online behaviour. I think  
80:37 it's more in relation to the way you leave 
data on the internet, which people can use,  
80:45 but maybe you could use digital 
footprint in relation to,  
80:50 you know, the kind of impact that you've had with 
your interactions online. And then etiquette.
80:59 Well, we have etiquette, which 
is like the rules of behaviour,   for example, polite behaviour. 
That's etiquette. For example,  
81:07 etiquette at the dinner table, that it's 
rude to put your elbows on the table. You have to, you know, eat your mouth,  
81:12 chew with your mouth closed. Don't talk with 
your mouth full. Say please and thank you. This is etiquette and etiquette is the same 
thing, but for the internet. So like rules,  
81:22 social rules for the internet. I mean, I remember 
netiquette used to be talked about a lot in the  
81:28 early days of the internet, but I don't think 
people talk about netiquette so much anymore,   which is maybe, you know, an indictment of the 
way that we have learned to use the internet.
81:41 Examples, I've seen some awful examples of 
cyber bullying on Instagram. Remember that  
81:46 your digital footprint lasts forever. 
So good netiquette is important. So be careful of the way that you interact 
with people and make sure that you,  
81:55 you know, that you stay polite. Let's see if I 
can maybe do the next three items in my list,  
82:01 which would take me up to seven 
and that would be halfway. That   would be a good point to stop and 
then maybe carry on in part two.
82:09 Okay. By the way, I will collect 
vocab from this episode and put  
82:14 it in a list at the end of the PDF. So 
all the bits of chunky vocab that have  
82:20 come up that I've explained 
quite quickly and moved on,   you'll be able to consolidate all that by looking 
at a vocab list, which you'll find on the PDF.
82:30 There will also be memory questions to 
help you jog your memory and recall bits   of vocabulary that have come up here. Hopefully 
you're discovering some new words and things  
82:41 during this episode and that you're able to 
use this to expand your vocabulary in order  
82:46 to be able to talk about all these things. I 
think it's important that you can talk about   these things in English because these are 
such important issues in our lives today.
82:56 Let's move on then to number five, 
which is digital legacy. Our lives   generate a massive amount of digital material, 
photos, emails, social media posts, documents,  
83:07 all stored on servers somewhere. But what happens 
to this digital legacy when we die or when we can  
83:14 no longer manage it ourselves? Planning for your 
digital assets means ensuring important memories  
83:20 are preserved, accounts are secured and private 
information doesn't fall into the wrong hands.
83:26 So do you keep a lot of things in the cloud? So 
photos, documents, music, personal data? Why and  
83:33 why not? Which cloud service do you use? Do you 
use iCloud, Google Drive, Dropbox or something  
83:39 else? And why do you use that particular service? 
So yeah, I do keep a lot of things in the cloud.  
83:48 I mean, when you think about digital legacy 
and all of the data that we all leave behind,  
83:54 this is not so much an issue of privacy, 
but more simply a case of the sheer volume  
83:59 of stuff that we all generate in our lives. It 
all has to be saved in the cloud, I suppose.
84:07 That's generally the way it's done these 
days. And we have to think about how we  
84:13 keep that secure. But also that all gets saved on 
servers that does actually occupy physical space.
84:20 Those servers have to be maintained. 
They have to be cooled with water.  
84:26 They do require a lot of energy to 
keep running, which is, you know,  
84:32 I mean, it's not great, is it that we are 
all filling up the world with our data.
84:38 But yeah, I keep things in the cloud. I mean,   I've got iCloud because I use Apple 
products. I've also got Google Drive.
84:46 Those are probably the main 
cloud storage systems that   I use. Which cloud service? Yeah. There you go.
84:55 Why do I use that particular service? Well, I 
use iCloud just because it's convenient as an   Apple user and you automatically get given a 
certain amount of storage. What else? And dry  
85:07 Google Drive I use because it's, you know, I find   it really convenient for Google documents. 
I don't know how secure they are really.
85:17 Anyway, relevant vocab, we talk about cloud 
storage, backup, making a backup of your data,  
85:25 digital inheritance. I suppose this relates to 
where does that data go when you die? And do you,  
85:32 you know, do you pass it on to like, you 
know, your loved ones? That's inheritance  
85:39 means the things that are given probably to your 
children when you die. That's your inheritance.
85:45 And normally we associate that with 
things like property, money, possessions,  
85:52 right? Your inheritance, the things you inherit 
from your parents probably, but then digital  
85:57 inheritance would be all the things that you pass 
on in the digital world. Two factor authentication  
86:06 is just a system to keep things more secure. 
You know, when you're signing it, when you sign  
86:12 into your Google Drive account, you need to, you 
need two ways to get in, you enter your password,  
86:19 but then you also have to receive an activation 
code on your phone or something like that. This is just like a security measure. 
We call it two factor authentication.  
86:30 Encryption relates to often like cloud 
services will not only store your data,   but encrypt it, meaning they 
will kind of make the data.
86:40 I don't know what they do, how they encrypt it, 
but I'm sure that I've got people listening to   this who work in this area, but something is 
done to the data. So it's kind of scrambled and  
86:51 it can only be unscrambled by the person 
who has full access to the account,   which means that the data is 
protected. It can't just be  
86:58 leaked that easily because it's been 
locked and encoded in a certain way.
87:06 And archives, archives are just like stores 
of old things. Like you have archives of old  
87:14 works of art, for example, or archives of 
Luke's English podcast episodes or archives  
87:19 of all your old photos and things like that. 
Managing your photos is a bit of a nightmare,  
87:25 isn't it? I don't know if you're an iPhone user, 
but that's a real pain in the neck, I have to say.
87:33 And you kind of think, what are Apple doing? Are 
they doing that on purpose? Because managing your  
87:39 photos, the problem is that your phone 
gets filled up with photos. We always,  
87:44 we all get that annoying moment where your 
phone slows down and you realise that it's   full. Storage on your phone is 
full and it's largely photos.
87:52 And you think, well, all these 
photos are in the cloud, right?   So if I just delete them from my phone, they're 
still in the cloud in my iCloud. So everything's  
88:00 fine. But the thing is, I've never worked 
out how this, how to stop this happening. If you delete photos on your phone, they also get 
deleted from the cloud, which seems to be the,  
88:13 you know, that's the opposite of what the point 
should be. You should be able to just back them  
88:19 up into the cloud and then delete them from your 
phone. But no, it's like it's all connected. So when you delete them on your phone, you also 
end up deleting them from the cloud. So I don't  
88:27 understand how this happens. And as a result, 
you end up having to like get a new phone.
88:33 I don't know. It's really confusing. 
Can you explain that to me? Anyway,   examples, I use cloud storage to keep 
a backup of all my family photos.
88:42 So a backup, that's a noun, a backup. Notice the 
stress is on the word back. So that's a backup.
88:50 The verb is to back something up. 
And in the phrasal verb, it's up,  
88:55 which is stressed. And this is true with 
lots of these examples of phrasal verbs  
89:01 and their noun equivalents, right? To 
back something up and to make a backup.
89:07 Right? In fact, I did a whole premium series all 
about this subject, phrasal verbs and their noun  
89:13 equivalents. I think it's P66, P65 is it? 
Premium listeners, you can confirm. But if  
89:22 you want to get a whole series about phrasal verbs 
and their noun equivalents, check out LEP premium.
89:28 Have you ever thought about your digital 
inheritance? What happens to your accounts   after you're gone? This is actually a really 
important thing. Personally, I do sometimes  
89:39 wonder about my digital inheritance, especially 
with my podcast stuff. I think if I died, right,  
89:46 what I would like, I would like someone to 
be able to publish at least one post or one  
89:53 episode to say, Oh yeah, by the way, sorry, 
you haven't had any podcasts for a while. It's just that Luke died. That's gonna, that will 
need to happen at some point. It's something worth  
90:06 thinking about, isn't it? And also we've 
probably all had that experience of sadly,  
90:11 like one of our friends dying, but their 
social media accounts are still alive,  
90:17 which is quite a, quite a disturbing phenomenon.
90:22 In fact, quite a good, good plot line for 
a horror movie where, you know, someone  
90:29 has died and it's very sad and you notice their 
social media account is still a lot, it's still   active. And then one day you get a message from 
that person. Whoa, good idea for a horror film.
90:40 Um, and the message is like, how are 
you? And I thought you were dead. No,  
90:46 I'm not dead. What? In fact, I'm standing outside 
your window and they're right there at the window.
90:54 Um, using encryption and two factor authentication 
makes your digital legacy safer. Yeah. Number six,  
91:00 tech addiction from endless social media feeds to 
the dopamine hit of a new notification technology  
91:07 is designed to keep us hooked while tech 
can make life easier and more entertaining.
91:12 It's also easy to slip into compulsive patterns, 
to slip into compulsive patterns. That's nice. Um,  
91:23 um, there are, there have been other bits 
of vocab, which I can add to my list. Uh, but that's quite good. Well, I'll come 
back to that in a second. Being aware of  
91:31 the signs of tech addiction helps us regain 
control and use technology in healthier ways.
91:37 So to slip into compulsive patterns, 
compulsive, if compulsive means to  
91:43 means doing things without really thinking them 
through fully. So if you are a compulsive person,  
91:51 you kind of just do things without really 
thinking about it. And we are quite compulsive.
91:58 I think most of us are probably 
quite compulsive with our, with our,   um, mobile phone usage, where, like I said 
before, you sit down at the end of the day  
92:07 on the sofa and the automatic thing to do 
is just to start scrolling through whatever   social media platform you use. And that's quite 
compulsive. You're not really thinking about it.
92:16 You just end up doing it. Like I do find 
myself doing that. I'll pick up my phone   in order to check the weather and I'm 
on some social media before I know it.
92:25 And I'm down a rabbit hole. And then I got, 
nevermind that, put my phone down. And then I'm   like, what was I doing? Oh yeah, I was checking 
the weather and I picked my phone up again.
92:34 So we are quite compulsive. Um, it's easy to 
slip into compulsive patterns mean to slip  
92:40 in means start doing those things without really 
intending to in the same way that you would slip  
92:48 if you're walking on, on, on, on ice, you would 
slip, you can slip into a pattern of behaviour,  
92:57 meaning you just kind of accidentally start doing 
that without intending to. Um, so tech addiction.
93:04 Okay. So do you know any examples 
of tech addiction? Yeah, I mean,   you know, like the most well 
known ones are things like, well,  
93:13 people just getting addicted to social media, 
spending unhealthy amounts of time on it, um,  
93:18 gaming as well, people spending hours and 
hours and hours playing certain games,  
93:23 insert, you know, online games, especially to the 
point where it becomes unhealthy. And addiction,  
93:30 I think is defined by any habit, which 
first of all is probably unhealthy,  
93:36 but also any habit where you lose that sense 
of control over when you do it and don't do it.
93:43 So any habit that becomes compulsive where you, 
you lose a sense of like when you can control,  
93:50 you know, you just feel you have to do it 
where you can't control when you want to do  
93:55 it. And the other one is when you stop doing 
it or when that habit is taken away from you,  
94:02 how do you respond? What, what kind of 
withdrawal symptoms do you experience? So  
94:07 if we talk about addiction to cigarettes, 
right, which is obviously unhealthy,  
94:13 but we do it anyway. It's compulsive in 
the sense that we are compelled to do it.
94:21 We don't necessarily want to, 
but we end up doing it anyway.   And when you take away the cigarettes, when that 
nicotine is taken away, we experience horrible  
94:32 physical and mental withdrawal symptoms. 
Withdrawal means taking the thing away.
94:38 And the symptoms of, you know, 
withdrawal from nicotine are,   you know, you've become very irritable. You can't 
focus and you get these incredible cravings where  
94:52 it's almost unbearable how much you need to 
get to, to smoke a cigarette in order to make  
94:58 the cravings go away. And it's the same story 
with, you know, many other substances as well.
95:04 But with tech addiction, it can be a similar 
thing where you can, let's say just your phone,  
95:10 just simply the act of going on your 
phone, you're compelled to do it. You   don't necessarily choose to do it. You just 
end up grabbing your phone and you're on it.
95:20 Like I said before, maybe it's bad for us 
because, you know, like I mentioned before,  
95:26 it's probably physically not great to be 
looking at to screen all the time. And  
95:31 mentally also is not very good for us probably. 
And also there are withdrawal symptoms often,  
95:38 you know, have you, I don't know if you've ever 
experienced it, but like, where suddenly that that  
95:45 option is taken away from you, how do you respond? 
Personally, I tend to respond quite well to it.
95:52 It makes me feel much more calm and healthy. But I 
think there are plenty of examples of people who,  
95:57 when the phone is taken away from 
them, they don't react to it very well.   And though, you know, you can see 
that tech addiction is a real thing.
96:05 So are you addicted even just 
a little bit to your phone,   to games or to the internet? How do you know? 
Yeah, I'm sure I am. I'm sure I'm addicted to,  
96:14 to this in the ways that I've described, maybe 
not that much. I think I've probably got it in   check, but I think there's probably a 
certain amount of addiction going on.
96:23 How do I know? Because I'm compelled to grab my 
phone. It's often the last thing I check at night,  
96:31 the first thing I check in the morning, 
which is quite sad, isn't it really? But   I think that I'm not the only one. I 
suppose that most of us are like that.
96:38 Have you ever tried to live without your 
device for a while? How do you feel if you   don't know where your phone is? Like 
my wife and I often talk about this.  
96:47 We say we should leave off. We should 
not have our phones in the bedroom. We should keep the phone somewhere else. We 
talk about like this, like it's a big life  
96:55 decision. Like should we have our, should we ban 
the phones from the bedroom? We totally should.
97:02 We know that we should, but 
we never do. How do I feel if   I don't know where my phone is? I don't 
feel great. Where's my phone? You know,  
97:11 you do panic, right? We do panic when 
we don't know where our phone is. Well, have you seen my phone? It's like, 
Oh God, where's my phone? We do tend to  
97:18 freak out. I remember not that long 
ago, I was out with my wife and kids.  
97:24 We were out having a day in Paris and 
we walked through some touristy areas.
97:29 I had like several layers of 
clothing on because it was   not hot. This is like in the near 
the end of winter. I think it was.
97:38 And at one point I realised I couldn't 
find my phone. I was checking my pockets,  
97:44 check my bag. Where is it? Oh my God. And I couldn't find it and I, I 
thought to myself, Oh no, it's been,  
97:52 Oh, I've been pick pocketed. I was convinced that 
someone had taken the phone out of my back pocket  
97:59 while I was walking through a touristy 
area. And instantly the day was ruined. It was like, no, not my phone. Oh God, right. 
We're going to have to go to the mobile phone  
98:09 shop, cancel the contract, cancel the SIM card, 
do this, do that, get a replacement phone.
98:14 It's like the day was ruined. And so we walked 
to the Metro station to find the nearest phone  
98:20 shop to do that. And the other it's 
like the day was completely ruined. As we were walking down into the Metro, I felt 
my inside pocket of my inside jacket and the  
98:30 phone was there. And the relief, the relief that I 
felt in that moment, like that rush of I guess of  
98:39 endorphins or whatever, the chemicals that flooded 
my brain, the relief I felt when I realised I  
98:45 had my phone, it was like the best high I'd had 
forever. You know, it was just an amazing feeling.
98:53 We all felt so much relief. And it 
then became the greatest day that  
98:59 we'd had for ages. And we went back 
out into the park and we're like,   you know what, let's go on that 
big, that observation wheel.
99:08 They have in the centre of Paris, in the 
park, they have this observation wheel,   a bit like the London Eye. And you 
can go up in the wheel, you can have  
99:16 a great view of the city from up there. We 
would never normally have chosen to do it.
99:22 You know, it's never something we would choose 
to do. But on that particular day we're like,   you know what, life just is worth living 
again. Let's go up on the observation wheel.
99:31 Why not? We went on the observation wheel. 
We had chocolate cookies and coffee and had  
99:38 the best day of our lives. But how do you feel 
if you don't know where your phone is? Well,  
99:44 we panic, don't we? We panic and we freak out. It's the end of the world. And then when we 
find it again, oh, such relief. What things  
99:54 make up most of your screen time? Which things 
hold your attention? For me, it's YouTube. I'm a bit of a YouTube junkie, as 
I said before. And also the YouTube  
100:03 studio checking out that list of 
which episodes are performing,  
100:09 you know, my recent episode. How is it 
performing? If it's near the top, I feel good.
100:15 If it's down at the bottom, I feel 
terrible. If it hits that top spot,   I get the confetti. Oh, that's good.
100:22 Dopamine is a chemical which 
is released by the brain.  
100:28 I understand it's like a reward 
chemical. It's a kind of narcotic,   I think, that your brain naturally produces 
that gives you a sort of reward feeling.
100:38 And we can become addicted to that. And certain 
stimulus causes our brain to release dopamine,  
100:46 which is, I suppose, how we end up being 
addicted to things on the internet. Compulsive. I've talked about scrolling. That's 
like going through things on the screen,  
100:56 making the screen go down or making the 
screen go up, which is what we do when   we're on social media. We scroll through 
different posts or videos or photos.
101:05 Binge watching. Binge watching is when we 
watch content online for a long period of time.  
101:11 For example, if you start watching 
a Netflix series at 10pm and it's   so fascinating and brilliant that you 
end up binge watching the whole series.
101:20 You stay up until three o'clock in the 
morning binge watching the show. That's   watching a lot of stuff in one sitting, 
let's say. We also have binge drinking.
101:31 This is where people go out and drink 
lots of alcoholic drinks in one go,   which is very unhealthy, and binge eating. That's 
where you might, I don't know, eat lots of stuff,  
101:43 lots of unhealthy. You've bought some ice cream 
and you know there's ice cream in the fridge. You might eat the whole tub of ice cream in one  
101:49 evening. That's bingeing on 
ice cream. Also, unhealthy. Withdrawal, I've talked about. That's 
where something is taken away from you,  
101:57 like whether it's cigarettes, alcohol, 
or access to your phone. A habit loop,  
102:06 that's where a loop is a thing 
that goes round and round. We can end up in a habit loop where we go in 
this cycle of reward, requiring that reward,  
102:17 getting it again and going back 
around and again. Trigger is where  
102:23 things trigger something. Social media can 
trigger a dopamine response in the brain.
102:32 That's, I suppose, what trigger means. Trigger 
means cause it to happen. We also use trigger on  
102:37 social media to refer to when someone becomes 
quickly emotional in response to something.
102:44 You can be triggered. That means 
to have a often negative emotional   response to something. So someone writes 
a comment and you instantly get angry.
102:53 That means you've been triggered by 
it. Examples, sometimes I find myself   scrolling mindlessly for hours without 
realising it. When I try to cut back,  
103:02 I actually feel withdrawal 
symptoms like restlessness. That's where you can't rest or relax. 
Notifications can act as a trigger,  
103:12 ping, pulling you into a habit loop of 
checking your phone. I mentioned before,  
103:17 like you want to check the weather. I want to check the weather. I look at my phone, 
oh, there's notification. Check the notification.
103:23 Then you end up in a rabbit hole on certain 
social media and then you're like, oh,   never mind. Put your phone down and then, oh,  
103:30 yeah, the weather and you're back in and then 
you're in that same habit loop. The trigger,  
103:35 the initial trigger was maybe a notification that 
you received that made you look at your phone.
103:40 So those are those things. Let's 
move on then to item seven,   which is the last item in part one here. 
And I'm assuming I'm going to do part two.
103:49 We will see. So number seven, tech for children. 
Today's children grow up surrounded by technology,  
103:56 often knowing how to swipe a 
tablet before they can read. This opens up huge opportunities for learning 
and connection, but also creates challenges  
104:06 around screen time, online safety and 
social development. Parents must decide  
104:11 how much access is healthy and how to protect 
their kids online. So questions, what do you  
104:17 think are the advantages and disadvantages 
of letting children have mobile phones?   Do you know any examples of children who 
have phones or access to the internet and  
104:27 how do the parents manage it? So one thing 
I should say is like, I mean, I'm a parent. I've got a daughter who's seven and 
I've got a son who's now two. It was  
104:36 his birthday not long ago. And this is 
something I think about all the time. And my wife and I both agree that we don't really 
want our kids to have phones until, you know,  
104:45 we want to leave it as late as possible. And this 
is based on information that we've read about the  
104:55 impact that phones can have on kids. And it's 
not just that they are sensitive and that they,  
105:03 you know, that it's not necessarily safe, 
but it's also because I've read reports  
105:09 about how mobile phones have caused a sort 
of mental health crisis among young people.
105:17 Maybe it's, maybe that it's unrelated. I 
don't know. But you can see in the data,  
105:24 the introduction of mobile phones 
and a spike in instances of bad  
105:30 mental health among young people, and 
they happen around about the same time. And it seems that the more kids have phones,  
105:38 the more mental health issues they have 
to deal with. Maybe it's not causation.  
105:44 Maybe one thing doesn't cause the other, but it's 
a pretty interesting correlation in any case.
105:52 And so, yeah, I'm really concerned that 
phones are not good for kids and certainly  
105:59 social media and kids is not really a very 
good idea. I think it's not very healthy  
106:06 for children's brains to be exposed 
to the sort of subtle manipulation,  
106:11 which happens with a lot of apps on phones and 
also the comparison culture, which happens where  
106:19 kids look at images of people. We think about 
images of the body, attitudes that are spread.
106:27 Children are very susceptible to 
social pressure and to an extent  
106:33 social media applications. That is 
their currency. It is social pressure.
106:38 I think it's perhaps dangerous for kids to be 
exposed to that kind of thing at a young age.  
106:45 It affects their mental development in the 
same way that you don't want your kids to be  
106:51 perhaps messing with drugs that will 
affect the way their brains work   or other habits that might seriously 
affect the way their brains work. You  
106:60 don't necessarily want them to be using phones 
that are manipulating, triggering their minds.
107:11 But that's not to say there are only 
disadvantages of letting children have   mobile phones. Of course there are advantages. 
You think about the fact that it means that us  
107:20 parents will be able to be in touch with the 
children and they will be in touch with us. So that is convenient and possibly 
safe. That it gives them a way to  
107:29 contact us if they're away from the house. 
And also letting kids use phones, tablets,  
107:36 computers is obviously really important to 
help them develop digital literacy so that  
107:41 they can use confidently, comfortably all these 
devices which are so important in our lives.
107:48 If the kids never use phones or 
computers they will grow up and   they won't know how to use them. And 
obviously that is really important  
107:56 in the professional world to be 
able to do that. So there's that.
108:03 I know a couple of examples of kids who have 
phones or access to the internet. You know   like I've got parents at the school who I 
know who have kids who are a bit older and  
108:13 they allow them to have phones but the phones have 
restricted internet access. So like flip phones,  
108:20 you know going back to the old fashioned 
phones that don't, they're not smartphones.
108:26 So the kids can make calls, 
they can send SMS text messages,   they might have limited access to WhatsApp 
perhaps. But essentially it's a numeric  
108:36 keypad like in the old days, you know when 
we all had Nokia or Sony Ericsson phones,  
108:44 they had numeric keyboards so you have to 
text by tapping the numbers a few times   to create the letters and there's no internet 
access. So that potentially allows the kids to  
108:57 have a certain amount of connectivity without 
exposing them to the dangers of the internet.
109:04 So there's that. But yeah we 
talk about parental controls  
109:10 where the parents are able to limit what the 
device can do. Screen time limits maybe just  
109:17 setting limits and time limits you know 
telling your children, setting a timer,  
109:22 telling the children they can only use 
their phones within a certain time period. Age restrictions, a lot of services on 
the internet provide age restrictions so  
109:33 that under a certain age you can't use this 
site or whatever. Cyber safety is obviously  
109:39 a really important thing because of course 
there are online predators who might try to  
109:46 groom children on social media. Grooming 
essentially means working on a child or a  
109:55 person and trying to manipulate their behaviour 
in order to then allow something else to happen.
110:01 For example you know you get online predators 
who will talk to children maybe pretending  
110:08 to be other children and they will be talking 
to them in a way that's like persuasive maybe  
110:14 preparing them in a way to then allow them 
to manipulate them, to allow them to meet  
110:21 in the real world where abuse might happen. 
This is known as grooming obviously a huge  
110:29 concern. Digital literacy I talked about 
and online predators I mentioned just now.
110:35 Many parents use parental controls to 
enforce screen time limits. Teaching   digital literacy helps kids understand 
what's safe online. Unfortunately the  
110:44 internet can expose children to online 
predators if they are not careful. It's a bit of a dark ending to this section but 
that's where we're going to end I guess what  
110:54 will be part one of this. As I said before 
you will find on the PDF a full vocab list  
111:01 of bits of vocab that have come up during 
the episode including the things I added   on my list there. Although I didn't add that 
many during the episode I ended up just sort  
111:08 of like focussing on talking and making my 
points clear but I did explain a few things. I'll put them all in the list. 
So you'll find a full vocab list  
111:15 for this episode. We'll also 
find memory recall questions. Little questions to help you actually 
recall the vocab which has come up which  
111:25 is a really important step. It's something 
I always do in premium episodes to help you  
111:30 not only understand notice identify know the 
meanings of words and phrases but to be able  
111:37 to actually come up with them yourself 
which is a really important step in you   remembering learning and then later using new 
vocab yourself. Let me know your comments.
111:46 You can answer any of the questions 
you've heard me talk about in this   episode. Let me know if you'd like to 
see part two. I mean I might I might  
111:57 actually have already recorded part 
two by the time I upload part one. We'll see how we go. But hopefully you would 
be interested in getting part two of this too.  
112:07 Hopefully you genuinely learned some new bits 
of vocab for these subjects and that having  
112:15 listened to this episode and hopefully worked 
with the PDF a bit you'll be in a much better   position to have conversations 
on these subjects in English.
112:25 But yes use all those questions yourselves. Answer   the questions yourselves. Don't feel 
like you have to give expert answers.
112:32 Just give your own answers about your 
own lives and see if you can use some   of the language that's come up. Okay. I think 
this is a chance for you to genuinely learn  
112:43 some very important vocab and to really 
expand your range of English for this subject.
112:50 Imagine next time you have to 
actually talk about this subject.   It's the sort of thing that could come up in 
a Cambridge exam like IELTS but just generally  
112:59 in conversation. Hopefully after using 
this episode you'll be in a much better  
113:05 position to have a good solid meaningful 
expressive conversation on this subject.
113:10 Okay. Leave your comments in the comments 
section. I'm always curious to get your responses.
113:16 But for this episode right here right now 
that is the end. Have a lovely morning   afternoon evening or night wherever you are 
out there in podcast land. Take care look  
113:27 after yourselves and each other and 
I will speak to you in the next one. But for now it's just time 
to say goodbye. Bye. Bye.